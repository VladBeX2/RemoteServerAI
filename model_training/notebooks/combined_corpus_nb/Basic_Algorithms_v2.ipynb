{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "try:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('punkt')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "   \n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", '[URL]', text)\n",
    "    \n",
    "    text = re.sub(r\"<.*?>+\", '[HTML]', text)\n",
    "    \n",
    "    text = re.sub(r'[#%&\\(\\)\\*\\+/:;<=>@\\[\\\\\\]^_`{|}~]', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def check_class_distribution(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    print(\"Distribuția claselor:\")\n",
    "    for label, count in distribution.items():\n",
    "        print(f\"Clasa {label}: {count} ({count/len(y)*100:.2f}%)\")\n",
    "    \n",
    "    minor_class_percentage = min(counts) / len(y) * 100\n",
    "    is_imbalanced = minor_class_percentage < 40\n",
    "    print(f\"Distribuție dezechilibrată: {is_imbalanced} (clasa minoritară: {minor_class_percentage:.2f}%)\")\n",
    "    return is_imbalanced\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "   \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Dimensiunea setului de antrenare\")\n",
    "    plt.ylabel(\"Acuratețe\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='accuracy')\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Acuratețe pe antrenare\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Acuratețe pe validare\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    diff = train_scores_mean[-1] - test_scores_mean[-1]\n",
    "    plt.annotate(f'Diferență: {diff:.4f}', \n",
    "                 xy=(train_sizes[-1], test_scores_mean[-1]),\n",
    "                 xytext=(train_sizes[-1] * 0.8, test_scores_mean[-1] - 0.1),\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    \n",
    "    return plt\n",
    "\n",
    "def train_evaluate_with_cv(vec_name, vectorizer, clf_name, clf, X, y, save_dir):\n",
    "   \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    X_processed = [preprocess_text(text) for text in X]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_processed, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "    \n",
    "    is_imbalanced = check_class_distribution(y_train)\n",
    "    \n",
    "    if vec_name == \"Bag_of_Words_(1-2gram)\":\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "    else:  \n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, \n",
    "                                    sublinear_tf=True, min_df=5)\n",
    "    \n",
    "    feature_selector = SelectKBest(chi2, k=3000)\n",
    "    \n",
    "    if is_imbalanced and clf_name not in [\"SVM\", \"KNN\"]: \n",
    "        pipeline = ImbPipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('feature_selection', feature_selector),\n",
    "            ('smote', SMOTE(random_state=RANDOM_SEED)),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = make_pipeline(\n",
    "            vectorizer,\n",
    "            feature_selector,\n",
    "            clf\n",
    "        )\n",
    "    \n",
    "    cv_accuracies = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_f1 = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1_weighted')\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    test_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    cv_test_diff = np.mean(cv_accuracies) - test_accuracy\n",
    "    \n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    learning_curve_title = f'Curba de învățare - {clf_name} cu {vec_name}'\n",
    "    plt = plot_learning_curve(\n",
    "        pipeline, learning_curve_title, X_train, y_train, cv=cv)\n",
    "    learning_curve_path = os.path.join(save_dir, f\"{clf_name}_{vec_name}_learning_curve.png\")\n",
    "    plt.savefig(learning_curve_path)\n",
    "    plt.close()\n",
    "    \n",
    "    filename = f\"{clf_name}_{vec_name}.joblib\".replace(\" \", \"_\")\n",
    "    model_path = os.path.join(save_dir, filename)\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    \n",
    "    print(f\"\\n--- Model: {clf_name} cu {vec_name} ---\")\n",
    "    print(f\"CV Acuratețe: {np.mean(cv_accuracies):.4f} (±{np.std(cv_accuracies):.4f})\")\n",
    "    print(f\"CV F1-Score: {np.mean(cv_f1):.4f} (±{np.std(cv_f1):.4f})\")\n",
    "    print(f\"Test Acuratețe: {test_accuracy:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"Diferența CV-Test: {cv_test_diff:.4f}\")\n",
    "    print(f\"Salvat la: {model_path}\")\n",
    "    \n",
    "    return {\n",
    "        \"vectorizer\": vec_name,\n",
    "        \"classifier\": clf_name,\n",
    "        \"cv_accuracy_mean\": float(np.mean(cv_accuracies)),\n",
    "        \"cv_accuracy_std\": float(np.std(cv_accuracies)),\n",
    "        \"cv_f1_mean\": float(np.mean(cv_f1)),\n",
    "        \"cv_f1_std\": float(np.std(cv_f1)),\n",
    "        \"test_accuracy\": float(test_accuracy),\n",
    "        \"test_f1\": float(test_f1),\n",
    "        \"test_precision\": float(test_precision),\n",
    "        \"test_recall\": float(test_recall),\n",
    "        \"cv_test_difference\": float(cv_test_diff),\n",
    "        \"classification_report\": classification_rep,\n",
    "        \"model_path\": model_path,\n",
    "        \"learning_curve_path\": learning_curve_path\n",
    "    }\n",
    "\n",
    "def plot_model_comparison(results, save_path):\n",
    "   \n",
    "    models = []\n",
    "    cv_scores = []\n",
    "    test_scores = []\n",
    "    differences = []\n",
    "    \n",
    "    for result in results:\n",
    "        model_name = f\"{result['classifier']} + {result['vectorizer']}\"\n",
    "        models.append(model_name)\n",
    "        cv_scores.append(result['cv_accuracy_mean'])\n",
    "        test_scores.append(result['test_accuracy'])\n",
    "        differences.append(result['cv_test_difference'])\n",
    "    \n",
    "    sorted_indices = np.argsort(cv_scores)[::-1]\n",
    "    models = [models[i] for i in sorted_indices]\n",
    "    cv_scores = [cv_scores[i] for i in sorted_indices]\n",
    "    test_scores = [test_scores[i] for i in sorted_indices]\n",
    "    differences = [differences[i] for i in sorted_indices]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    ax1.bar(x - width/2, cv_scores, width, label='CV Acuratețe', color='skyblue')\n",
    "    ax1.bar(x + width/2, test_scores, width, label='Test Acuratețe', color='lightgreen')\n",
    "    \n",
    "    ax1.set_title('Comparație Acuratețe și Diferență CV-Test', fontsize=16)\n",
    "    ax1.set_ylabel('Acuratețe', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right', fontsize=12)\n",
    "    ax1.legend(loc='upper left', fontsize=12)\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, differences, 'ro-', linewidth=2, markersize=8, label='Diferență CV-Test')\n",
    "    ax2.set_ylabel('Diferență CV-Test', color='r', fontsize=14)\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax2.axhline(y=0.05, color='r', linestyle='--', alpha=0.7)\n",
    "    ax2.text(x[-1], 0.05, '  Prag overfitting (0.05)', color='r', va='center')\n",
    "    \n",
    "    for i, v in enumerate(cv_scores):\n",
    "        ax1.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    for i, v in enumerate(test_scores):\n",
    "        ax1.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    for i, v in enumerate(differences):\n",
    "        ax2.text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom', color='r', fontsize=10)\n",
    "    \n",
    "    ax2.legend(loc='upper right', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def analyze_dataset(data):\n",
    "   \n",
    "    print(f\"Număr total de înregistrări: {data.shape[0]}\")\n",
    "    \n",
    "    print(f\"Coloane disponibile: {data.columns.tolist()}\")\n",
    "    \n",
    "    class_dist = data['Label'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nDistribuția claselor:\")\n",
    "    print(class_dist)\n",
    "    \n",
    "    data['text_length'] = data['Statement'].str.len()\n",
    "    \n",
    "    print(f\"\\nLungimea medie a textelor: {data['text_length'].mean():.2f} caractere\")\n",
    "    print(f\"Lungimea minimă: {data['text_length'].min()} caractere\")\n",
    "    print(f\"Lungimea maximă: {data['text_length'].max()} caractere\")\n",
    "    \n",
    "    print(\"\\nLungimea medie a textelor pe clase:\")\n",
    "    print(data.groupby('Label')['text_length'].mean())\n",
    "    \n",
    "    print(\"\\nVerificare cuvinte comune în fiecare clasă...\")\n",
    "    \n",
    "    def get_common_words(texts, n=20):\n",
    "        all_words = ' '.join(texts).split()\n",
    "        from collections import Counter\n",
    "        return Counter(all_words).most_common(n)\n",
    "    \n",
    "    fake_samples = data[data['Label'] == 0]['Statement'].sample(min(1000, sum(data['Label'] == 0)))\n",
    "    real_samples = data[data['Label'] == 1]['Statement'].sample(min(1000, sum(data['Label'] == 1)))\n",
    "    \n",
    "    processed_fake = [preprocess_text(text) for text in fake_samples]\n",
    "    processed_real = [preprocess_text(text) for text in real_samples]\n",
    "    \n",
    "    print(\"\\nCele mai comune cuvinte în știri false:\")\n",
    "    print(get_common_words(processed_fake))\n",
    "    \n",
    "    print(\"\\nCele mai comune cuvinte în știri reale:\")\n",
    "    print(get_common_words(processed_real))\n",
    "    \n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    \n",
    "    vectorizer = HashingVectorizer(n_features=1000)\n",
    "    X_hashed = vectorizer.transform(data['Statement'])\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    \n",
    "    sample_size = min(1000, data.shape[0])\n",
    "    sample_indices = np.random.choice(data.shape[0], sample_size, replace=False)\n",
    "    X_sample = X_hashed[sample_indices]\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(X_sample)\n",
    "    \n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    \n",
    "    high_similarity = np.where(similarity_matrix > 0.9)\n",
    "    \n",
    "    print(f\"\\nNumărul de perechi de texte cu similaritate > 0.9: {len(high_similarity[0])}\")\n",
    "    \n",
    "    if len(high_similarity[0]) > 0:\n",
    "        print(\"\\nExemple de texte foarte similare:\")\n",
    "        for i in range(min(5, len(high_similarity[0]))):\n",
    "            idx1, idx2 = sample_indices[high_similarity[0][i]], sample_indices[high_similarity[1][i]]\n",
    "            print(f\"\\nText 1 (Label {data.iloc[idx1]['Label']}):\")\n",
    "            print(data.iloc[idx1]['Statement'][:100] + \"...\")\n",
    "            print(f\"Text 2 (Label {data.iloc[idx2]['Label']}):\")\n",
    "            print(data.iloc[idx2]['Statement'][:100] + \"...\")\n",
    "    \n",
    "    return {\n",
    "        'total_records': data.shape[0],\n",
    "        'class_distribution': class_dist.to_dict(),\n",
    "        'avg_text_length': data['text_length'].mean(),\n",
    "        'similar_texts_count': len(high_similarity[0])\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    save_dir = \"../saved_models/optimized\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    data = pd.read_csv(\"../../datasets/Combined_Corpus/All.csv\")\n",
    "    print(f\"Dimensiunea inițială a datelor: {data.shape}\")\n",
    "    \n",
    "    data = data[data['word_count'] >= 30]\n",
    "    print(f\"Dimensiunea după filtrare: {data.shape}\")\n",
    "    \n",
    "    dataset_analysis = analyze_dataset(data)\n",
    "    \n",
    "    vectorizers = {\n",
    "        \"Bag_of_Words_(1-2gram)\": None, \n",
    "        \"TFIDF_(1-2gram)\": None \n",
    "    }\n",
    "    \n",
    "    classifiers = {\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            max_depth=20,  \n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,  \n",
    "            max_features='sqrt', \n",
    "            random_state=RANDOM_SEED,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        \"LogisticRegression\": LogisticRegression(\n",
    "            C=1.0,  \n",
    "            max_iter=1000,\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "        \"NaiveBayes\": MultinomialNB(\n",
    "            alpha=0.5  \n",
    "        ),\n",
    "        \"SVM\": SVC(\n",
    "            C=1.0,  \n",
    "            kernel='linear', \n",
    "            probability=True,\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "        \"KNN\": KNeighborsClassifier(\n",
    "            n_neighbors=5,\n",
    "            weights='distance',  \n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    X = data['Statement'].values\n",
    "    y = data['Label'].values\n",
    "    \n",
    "    tasks = []\n",
    "    for vec_name in vectorizers.keys():\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            tasks.append((vec_name, None, clf_name, clf))\n",
    "    \n",
    "    if len(tasks) <= 2:\n",
    "        results = []\n",
    "        for vec_name, vec, clf_name, clf in tasks:\n",
    "            result = train_evaluate_with_cv(vec_name, vec, clf_name, clf, X, y, save_dir)\n",
    "            results.append(result)\n",
    "    else:\n",
    "        results = Parallel(n_jobs=2)(\n",
    "            delayed(train_evaluate_with_cv)(vec_name, vec, clf_name, clf, X, y, save_dir)\n",
    "            for vec_name, vec, clf_name, clf in tasks\n",
    "        )\n",
    "    \n",
    "    results_summary = {\n",
    "        \"dataset_analysis\": dataset_analysis,\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    results_file = os.path.join(save_dir, \"results_summary_optimized.json\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(results_summary, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nRezultatele au fost salvate în {results_file}\")\n",
    "    \n",
    "    plot_model_comparison(results, os.path.join(save_dir, \"model_comparison.png\"))\n",
    "    \n",
    "    min_diff_idx = np.argmin([r['cv_test_difference'] for r in results])\n",
    "    best_model = results[min_diff_idx]\n",
    "    \n",
    "    print(\"\\n==== Cel mai bun model (diferență minimă între CV și testare) ====\")\n",
    "    print(f\"Model: {best_model['classifier']} cu {best_model['vectorizer']}\")\n",
    "    print(f\"CV Acuratețe: {best_model['cv_accuracy_mean']:.4f}\")\n",
    "    print(f\"Test Acuratețe: {best_model['test_accuracy']:.4f}\")\n",
    "    print(f\"Diferența: {best_model['cv_test_difference']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
