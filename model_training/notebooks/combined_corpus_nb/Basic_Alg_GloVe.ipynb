{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", '', text)\n",
    "    text = re.sub(r\"<.*?>+\", '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)  # eliminare cuvinte cu cifre\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    text = re.sub(r'[“”‘’]', '', text)  \n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path):\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_transform(texts, embeddings_index, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Transforma fiecare text într-un vector GloVe de dimensiune embedding_dim\n",
    "    prin medierea embedding-urilor cuvintelor (care exista în embeddings_index).\n",
    "    Returneaza un array de dimensiune [num_samples, embedding_dim].\n",
    "    \"\"\"\n",
    "    X_vectors = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        valid_vectors = []\n",
    "        for token in tokens:\n",
    "            if token in embeddings_index:\n",
    "                valid_vectors.append(embeddings_index[token])\n",
    "        if len(valid_vectors) == 0:\n",
    "            # fallback: vector de zero daca niciun cuvant nu e in vocabular\n",
    "            X_vectors.append(np.zeros(embedding_dim, dtype='float32'))\n",
    "        else:\n",
    "            # media embedding-urilor\n",
    "            X_vectors.append(np.mean(valid_vectors, axis=0))\n",
    "    return np.array(X_vectors, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(vec_name, X_train_vec, X_test_vec, clf_name, clf, y_train, y_test, save_dir):\n",
    "   \n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    filename = f\"{clf_name}_{vec_name}.joblib\".replace(\" \", \"_\")\n",
    "    model_path = os.path.join(save_dir, filename)\n",
    "    joblib.dump(clf, model_path)\n",
    "    print(f\"Saved model: {model_path} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"vectorizer\": vec_name,\n",
    "        \"classifier\": clf_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"report\": report,\n",
    "        \"model_path\": model_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma inițială a dataset-ului: (86531, 6)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../../datasets/Combined_Corpus/All.csv\")\n",
    "print(\"Forma initiala a dataset-ului:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "După filtrare (word_count >= 30): (82540, 6)\n"
     ]
    }
   ],
   "source": [
    "data = data[data['word_count'] >= 30]\n",
    "print(\"Dupa filtrare (word_count >= 30):\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Statement'] = data['Statement'].apply(wordopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Statement'].values\n",
    "y = data['Label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings from ../../datasets/GloVe_embeddings/glove.6B.300d.txt...\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"../../datasets/GloVe_embeddings/glove.6B.300d.txt\"  \n",
    "embedding_dim = 300\n",
    "print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
    "embeddings_index = load_glove_embeddings(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming texts into GloVe vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transforming texts into GloVe vectors...\")\n",
    "X_train_glove = glove_transform(X_train, embeddings_index, embedding_dim)\n",
    "X_test_glove = glove_transform(X_test, embeddings_index, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "vec_name = \"GloVe_300d\"  \n",
    "for clf_name, clf in classifiers.items():\n",
    "    tasks.append((vec_name, X_train_glove, X_test_glove, clf_name, clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model: saved_models/NaiveBayes_GloVe_300d.joblib | Accuracy: 0.6603\n",
      "Saved model: saved_models/LogisticRegression_GloVe_300d.joblib | Accuracy: 0.8655\n",
      "Saved model: saved_models/KNN_GloVe_300d.joblib | Accuracy: 0.8091\n",
      "Saved model: saved_models/RandomForest_GloVe_300d.joblib | Accuracy: 0.8726\n",
      "Saved model: saved_models/SVM_GloVe_300d.joblib | Accuracy: 0.9141\n"
     ]
    }
   ],
   "source": [
    "results = Parallel(n_jobs=4)(\n",
    "        delayed(train_and_evaluate)(\n",
    "            vec_name, X_train_vec, X_test_vec, clf_name, clf, y_train, y_test, save_dir\n",
    "        )\n",
    "        for (vec_name, X_train_vec, X_test_vec, clf_name, clf) in tasks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary saved to saved_models/GloVe_results_summary.json\n"
     ]
    }
   ],
   "source": [
    "results_summary = {\"results\": results}\n",
    "results_file = os.path.join(save_dir, \"GloVe_results_summary.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=4)\n",
    "print(f\"Results summary saved to {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proiect_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
