{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Algorithms trained on Combined Corpus dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vlad.cristescu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/vlad.cristescu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/vlad.cristescu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/vlad.cristescu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "  text=text.lower()\n",
    "  text=re.sub('\\[.*?\\]','',text)\n",
    "  text = re.sub(r\"https?://\\S+|www\\.\\S+\", '', text)\n",
    "  text = re.sub(r\"<.*?>+\", '', text)\n",
    "  text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "  text = re.sub(r'\\n', '', text)\n",
    "  text = re.sub(r'\\w*\\d\\w*', '', text)  # eliminare cuvinte cu cifre\n",
    "  text = re.sub(r'\\s+', ' ', text).strip() \n",
    "  text = re.sub(r'[“”‘’]', '', text)  \n",
    "\n",
    "\n",
    "  text = \" \".join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
    "\n",
    "  return text\n",
    "\n",
    "def train_and_evaluate(vec_name, vectorizer, clf_name, clf, X_train, X_test, y_train, y_test, save_dir):\n",
    "   \n",
    "    pipe = make_pipeline(vectorizer, clf)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    filename = f\"{clf_name}_{vec_name}.joblib\".replace(\" \", \"_\")\n",
    "    model_path = os.path.join(save_dir, filename)\n",
    "    joblib.dump(pipe, model_path)\n",
    "    print(f\"Saved model: {model_path} | Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"vectorizer\": vec_name,\n",
    "        \"classifier\": clf_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"report\": report,\n",
    "        \"model_path\": model_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"../saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"../../datasets/Combined_Corpus/All.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Label</th>\n",
       "      <th>Statement_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>More Than 100 Million Americans Are On Welfare...</td>\n",
       "      <td>0</td>\n",
       "      <td>4669</td>\n",
       "      <td>833</td>\n",
       "      <td>3836</td>\n",
       "      <td>4.605042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There is no commitment to provide players, and...</td>\n",
       "      <td>1</td>\n",
       "      <td>2291</td>\n",
       "      <td>387</td>\n",
       "      <td>1904</td>\n",
       "      <td>4.919897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does Uptick in Mysterious Booms Foretell Mega-...</td>\n",
       "      <td>0</td>\n",
       "      <td>8810</td>\n",
       "      <td>1450</td>\n",
       "      <td>7360</td>\n",
       "      <td>5.075862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rand Paul Exposes the Crony Federal Reserve on...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>5.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andrew S. Grove, the longtime chief executive ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12473</td>\n",
       "      <td>1980</td>\n",
       "      <td>10494</td>\n",
       "      <td>5.300000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Statement  Label  Statement_length  \\\n",
       "0  More Than 100 Million Americans Are On Welfare...      0              4669   \n",
       "1  There is no commitment to provide players, and...      1              2291   \n",
       "2  Does Uptick in Mysterious Booms Foretell Mega-...      0              8810   \n",
       "3  Rand Paul Exposes the Crony Federal Reserve on...      0                73   \n",
       "4  Andrew S. Grove, the longtime chief executive ...      1             12473   \n",
       "\n",
       "   word_count  char_count  avg_word_length  \n",
       "0         833        3836         4.605042  \n",
       "1         387        1904         4.919897  \n",
       "2        1450        7360         5.075862  \n",
       "3          12          61         5.083333  \n",
       "4        1980       10494         5.300000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86531, 6)\n",
      "(82540, 6)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data = data[data['word_count'] >= 30]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Statement'] = data['Statement'].apply(wordopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Statement'].values\n",
    "y = data['Label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = {\n",
    "    \"Bag_of_Words_(1-3gram)\": CountVectorizer(ngram_range=(1, 3),max_features=20000),\n",
    "    \"TFIDF_(1-3gram)\": TfidfVectorizer(ngram_range=(1, 3),max_features=20000)\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"NaiveBayes\": MultinomialNB(),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = []\n",
    "for vec_name, vectorizer in vectorizers.items():\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        tasks.append((vec_name, vectorizer, clf_name, clf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model: ../saved_models/NaiveBayes_Bag_of_Words_(1-3gram).joblib | Accuracy: 0.8976\n",
      "Saved model: ../saved_models/LogisticRegression_Bag_of_Words_(1-3gram).joblib | Accuracy: 0.9771\n",
      "Saved model: ../saved_models/RandomForest_Bag_of_Words_(1-3gram).joblib | Accuracy: 0.9653\n",
      "Saved model: ../saved_models/RandomForest_TFIDF_(1-3gram).joblib | Accuracy: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad.cristescu/miniconda3/envs/proiect_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model: ../saved_models/KNN_Bag_of_Words_(1-3gram).joblib | Accuracy: 0.7463\n",
      "Saved model: ../saved_models/LogisticRegression_TFIDF_(1-3gram).joblib | Accuracy: 0.9758\n",
      "Saved model: ../saved_models/NaiveBayes_TFIDF_(1-3gram).joblib | Accuracy: 0.9466\n",
      "Saved model: ../saved_models/KNN_TFIDF_(1-3gram).joblib | Accuracy: 0.8493\n",
      "Saved model: ../saved_models/SVM_Bag_of_Words_(1-3gram).joblib | Accuracy: 0.9627\n",
      "Saved model: ../saved_models/SVM_TFIDF_(1-3gram).joblib | Accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "results = Parallel(n_jobs=4)(\n",
    "    delayed(train_and_evaluate)(vec_name, vectorizer, clf_name, clf,\n",
    "                                X_train, X_test, y_train, y_test, save_dir)\n",
    "    for vec_name, vectorizer, clf_name, clf in tasks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_name = \"TFIDF_(1-3gram)\"\n",
    "# clf_name = \"SVM\"\n",
    "# vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "# clf = SVC(random_state=42)\n",
    "\n",
    "# # Directorul de salvare\n",
    "# save_dir = \"saved_models\"\n",
    "\n",
    "# train_and_evaluate(vec_name, vectorizer, clf_name, clf,\n",
    "#                    X_train, X_test, y_train, y_test, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary saved to ../saved_models/results_summary.json\n"
     ]
    }
   ],
   "source": [
    "results_summary = {\"results\": results}\n",
    "results_file = os.path.join(save_dir, \"results_summary.json\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=4)\n",
    "print(f\"Results summary saved to {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proiect_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
