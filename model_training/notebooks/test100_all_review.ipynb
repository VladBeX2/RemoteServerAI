{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8ffb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged 24 files into merged_teste100_predictions.tsv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "# Folder where your individual inference TSVs live:\n",
    "PRED_DIR = \"../datasets\"\n",
    "# Glob pattern to match all your teste100_*_predictions.tsv files:\n",
    "PATTERN  = os.path.join(PRED_DIR, \"teste100_*_predictions.tsv\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ MERGE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "files = sorted(glob.glob(PATTERN))\n",
    "if not files:\n",
    "    print(\"‚ùå No prediction files found:\", PATTERN)\n",
    "    exit(1)\n",
    "\n",
    "merged_df = None\n",
    "\n",
    "for path in files:\n",
    "    fname = os.path.basename(path)\n",
    "    m = re.match(r\"teste100_(.*)_predictions\\.tsv$\", fname)\n",
    "    if not m:\n",
    "        print(f\"‚ö†Ô∏è  Skipping unexpected file name: {fname}\")\n",
    "        continue\n",
    "    model_name = m.group(1)\n",
    "\n",
    "    # load\n",
    "    df = pd.read_csv(path, sep=\"\\t\", dtype=str)\n",
    "    # initialize on first file\n",
    "    if merged_df is None:\n",
    "        merged_df = df[[\"text\", \"label(1=real,0=fake)\"]].copy()\n",
    "\n",
    "    # add columns\n",
    "    merged_df[f\"{model_name}_predicted\"]      = df[\"predicted\"]\n",
    "    # some older scripts may have None or missing prob column\n",
    "    if \"predicted_prob\" in df.columns:\n",
    "        merged_df[f\"{model_name}_predicted_prob\"] = df[\"predicted_prob\"]\n",
    "    else:\n",
    "        merged_df[f\"{model_name}_predicted_prob\"] = \"\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ WRITE OUT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "out_path = \"../datasets/merged_teste100_predictions.tsv\"\n",
    "merged_df.to_csv(out_path, sep=\"\\t\", index=False)\n",
    "print(f\"‚úÖ Merged {len(files)} files into {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fd5948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Worst FAKE article (only 8/24 models got it right) ---\n",
      "Text: The @DOGE team just discovered that FEMA sent $59M LAST WEEK to luxury hotels in New York City to house illegal migrants. Sending this money violated the law and is in gross insubordination to the President‚Äôs executive order. That money is meant for American disaster relief and instead is being spent on high end hotels for illegals! A clawback demand will be made today to recoup those funds.\n",
      "\n",
      "Models that guessed it correctly:\n",
      "  ‚Ä¢ KNN_GloVe_300d\n",
      "  ‚Ä¢ LSTM_GloVe\n",
      "  ‚Ä¢ LSTM\n",
      "  ‚Ä¢ RandomForest_GloVe_300d\n",
      "  ‚Ä¢ SVM_Bag_of_Words_(1-3gram)\n",
      "  ‚Ä¢ bert_v2\n",
      "  ‚Ä¢ roberta_adversarial\n",
      "  ‚Ä¢ roberta_v3\n",
      "\n",
      "--- Worst REAL article (only 4/24 models got it right) ---\n",
      "Text: Democrats mistakenly tweet 2014 pictures from Obama‚Äôs term showing children from the Border in steel cages. They thought it was recent pictures in order to make us look bad, but backfires. Dems must agree to Wall and new Border Protection for good of country...Bipartisan Bill!\n",
      "\n",
      "Models that guessed it correctly:\n",
      "  ‚Ä¢ LogisticRegression_GloVe_300d\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)\n",
      "  ‚Ä¢ SVM_GloVe_300d\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "MERGED_TSV = \"../datasets/merged_teste100_predictions.tsv\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ LOAD & SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "df = pd.read_csv(MERGED_TSV, sep=\"\\t\", dtype=str)\n",
    "\n",
    "# Identify the prediction columns (everything ending with \"_predicted\")\n",
    "pred_cols = [c for c in df.columns if c.endswith(\"_predicted\")]\n",
    "\n",
    "# Normalize the true label to \"REAL\"/\"FAKE\"\n",
    "df[\"true_label\"] = (\n",
    "    df[\"label(1=real,0=fake)\"]\n",
    "      .str.strip()\n",
    "      .map({\"1\":\"REAL\",\"__LABEL__1\":\"REAL\",\"REAL\":\"REAL\",\n",
    "            \"0\":\"FAKE\",\"__LABEL__0\":\"FAKE\",\"FAKE\":\"FAKE\"})\n",
    ")\n",
    "\n",
    "# Compute a ‚Äúcorrect‚Äù boolean for each model\n",
    "for col in pred_cols:\n",
    "    df[col + \"_correct\"] = (df[col] == df[\"true_label\"])\n",
    "\n",
    "# Sum how many models got each row right\n",
    "correct_cols = [c for c in df.columns if c.endswith(\"_correct\")]\n",
    "df[\"correct_count\"] = df[correct_cols].sum(axis=1).astype(int)\n",
    "\n",
    "# Total number of models\n",
    "n_models = len(pred_cols)\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ FIND THE WORST ARTICLE PER CLASS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "def report_worst(label):\n",
    "    sub = df[df[\"true_label\"] == label]\n",
    "    # find the minimum correct_count\n",
    "    min_corr = sub[\"correct_count\"].min()\n",
    "    worst = sub[sub[\"correct_count\"] == min_corr].iloc[0]\n",
    "    models_correct = [col.replace(\"_predicted_correct\",\"\") \n",
    "                      for col in correct_cols \n",
    "                      if worst[col]]\n",
    "    print(f\"\\n--- Worst {label} article (only {min_corr}/{n_models} models got it right) ---\")\n",
    "    print(f\"Text: {worst['text']}\\n\")\n",
    "    print(\"Models that guessed it correctly:\")\n",
    "    print(\"\\n\".join(f\"  ‚Ä¢ {m}\" for m in models_correct))\n",
    "\n",
    "report_worst(\"FAKE\")\n",
    "report_worst(\"REAL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62335429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                     |    Acc |  Prec_G |   Rec_G |    F1_G | Prec_FAKE |  Rec_FAKE | F1_FAKE | Prec_REAL |  Rec_REAL | F1_REAL\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "CNN_GloVe                 | 84.00% |  85.42% |  84.00% |  83.84% |    78.33% |    94.00% |  85.45% |    92.50% |    74.00% |  82.22%\n",
      "CNN_LSTM_GloVe            | 82.00% |  84.72% |  82.00% |  81.64% |    75.00% |    96.00% |  84.21% |    94.44% |    68.00% |  79.07%\n",
      "CNN_LSTM                  | 82.00% |  84.72% |  82.00% |  81.64% |    75.00% |    96.00% |  84.21% |    94.44% |    68.00% |  79.07%\n",
      "CNN                       | 84.00% |  85.42% |  84.00% |  83.84% |    78.33% |    94.00% |  85.45% |    92.50% |    74.00% |  82.22%\n",
      "KNN_Bag_of_Words_(1-3gram) | 60.00% |  60.15% |  60.00% |  59.86% |    61.36% |    54.00% |  57.45% |    58.93% |    66.00% |  62.26%\n",
      "KNN_GloVe_300d            | 56.00% |  56.37% |  56.00% |  55.36% |    57.89% |    44.00% |  50.00% |    54.84% |    68.00% |  60.71%\n",
      "KNN_TFIDF_(1-3gram)       | 55.00% |  57.06% |  55.00% |  51.46% |    60.87% |    28.00% |  38.36% |    53.25% |    82.00% |  64.57%\n",
      "LSTM_GloVe                | 90.00% |  91.05% |  90.00% |  89.94% |    84.48% |    98.00% |  90.74% |    97.62% |    82.00% |  89.13%\n",
      "LSTM                      | 90.00% |  91.05% |  90.00% |  89.94% |    84.48% |    98.00% |  90.74% |    97.62% |    82.00% |  89.13%\n",
      "LogisticRegression_Bag_of_Words_(1-3gram) | 55.00% |  69.20% |  55.00% |  44.79% |    85.71% |    12.00% |  21.05% |    52.69% |    98.00% |  68.53%\n",
      "LogisticRegression_GloVe_300d | 88.00% |  90.32% |  88.00% |  87.82% |   100.00% |    76.00% |  86.36% |    80.65% |   100.00% |  89.29%\n",
      "LogisticRegression_TFIDF_(1-3gram) | 61.00% |  74.31% |  61.00% |  54.81% |    92.31% |    24.00% |  38.10% |    56.32% |    98.00% |  71.53%\n",
      "NaiveBayes_Bag_of_Words_(1-3gram) | 50.00% |  25.00% |  50.00% |  33.33% |     0.00% |     0.00% |   0.00% |    50.00% |   100.00% |  66.67%\n",
      "NaiveBayes_GloVe_300d     | 67.00% |  67.57% |  67.00% |  66.73% |    70.73% |    58.00% |  63.74% |    64.41% |    76.00% |  69.72%\n",
      "NaiveBayes_TFIDF_(1-3gram) | 50.00% |  25.00% |  50.00% |  33.33% |     0.00% |     0.00% |   0.00% |    50.00% |   100.00% |  66.67%\n",
      "RandomForest_Bag_of_Words_(1-3gram) | 78.00% |  78.74% |  78.00% |  77.86% |    83.33% |    70.00% |  76.09% |    74.14% |    86.00% |  79.63%\n",
      "RandomForest_GloVe_300d   | 83.00% |  83.66% |  83.00% |  82.92% |    88.37% |    76.00% |  81.72% |    78.95% |    90.00% |  84.11%\n",
      "RandomForest_TFIDF_(1-3gram) | 81.00% |  81.31% |  81.00% |  80.95% |    84.44% |    76.00% |  80.00% |    78.18% |    86.00% |  81.90%\n",
      "SVM_Bag_of_Words_(1-3gram) | 72.00% |  77.28% |  72.00% |  70.58% |    89.29% |    50.00% |  64.10% |    65.28% |    94.00% |  77.05%\n",
      "SVM_GloVe_300d            | 95.00% |  95.02% |  95.00% |  95.00% |    94.12% |    96.00% |  95.05% |    95.92% |    94.00% |  94.95%\n",
      "SVM_TFIDF_(1-3gram)       | 56.00% |  70.38% |  56.00% |  46.58% |    87.50% |    14.00% |  24.14% |    53.26% |    98.00% |  69.01%\n",
      "bert_v2                   | 90.00% |  90.26% |  90.00% |  89.98% |    87.04% |    94.00% |  90.38% |    93.48% |    86.00% |  89.58%\n",
      "roberta_adversarial       | 91.00% |  91.41% |  91.00% |  90.98% |    95.56% |    86.00% |  90.53% |    87.27% |    96.00% |  91.43%\n",
      "roberta_v3                | 92.00% |  92.07% |  92.00% |  92.00% |    93.75% |    90.00% |  91.84% |    90.38% |    94.00% |  92.16%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ LOAD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "df = pd.read_csv(MERGED_TSV, sep=\"\\t\", dtype=str)\n",
    "\n",
    "# identify all the model‚Äêprediction columns\n",
    "models = [c[:-10]  # strip \"_predicted\"\n",
    "          for c in df.columns \n",
    "          if c.endswith(\"_predicted\")]\n",
    "\n",
    "y_true = df[\"label(1=real,0=fake)\"].str.strip().map({\n",
    "    \"1\":\"REAL\",\"__LABEL__1\":\"REAL\",\"REAL\":\"REAL\",\n",
    "    \"0\":\"FAKE\",\"__LABEL__0\":\"FAKE\",\"FAKE\":\"FAKE\"\n",
    "}).tolist()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ COMPUTE & DISPLAY ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "print(f\"{'Model':25} | {'Acc':>6} | {'Prec_G':>7} | {'Rec_G':>7} | {'F1_G':>7} |\"\n",
    "      f\" {'Prec_FAKE':>9} | {'Rec_FAKE':>9} | {'F1_FAKE':>7} |\"\n",
    "      f\" {'Prec_REAL':>9} | {'Rec_REAL':>9} | {'F1_REAL':>7}\")\n",
    "print(\"-\"*110)\n",
    "\n",
    "for m in models:\n",
    "    y_pred = df[f\"{m}_predicted\"].tolist()\n",
    "    # general (macro)\n",
    "    acc_g   = accuracy_score(y_true, y_pred)\n",
    "    prec_g  = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec_g   = recall_score   (y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_g    = f1_score       (y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    # fake as positive\n",
    "    prec_f  = precision_score(y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    rec_f   = recall_score   (y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    f1_f    = f1_score       (y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    # real as positive\n",
    "    prec_r  = precision_score(y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "    rec_r   = recall_score   (y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "    f1_r    = f1_score       (y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "\n",
    "    print(f\"{m:25} |\"\n",
    "          f\" {acc_g*100:5.2f}% |\"\n",
    "          f\" {prec_g*100:6.2f}% |\"\n",
    "          f\" {rec_g*100:6.2f}% |\"\n",
    "          f\" {f1_g*100:6.2f}% |\"\n",
    "          f\" {prec_f*100:8.2f}% |\"\n",
    "          f\" {rec_f*100:8.2f}% |\"\n",
    "          f\" {f1_f*100:6.2f}% |\"\n",
    "          f\" {prec_r*100:8.2f}% |\"\n",
    "          f\" {rec_r*100:8.2f}% |\"\n",
    "          f\" {f1_r*100:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0a29ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Top 5 by Accuracy (general):\n",
      "  ‚Ä¢ SVM_GloVe_300d                  95.00%\n",
      "  ‚Ä¢ roberta_v3                      92.00%\n",
      "  ‚Ä¢ roberta_adversarial             91.00%\n",
      "  ‚Ä¢ LSTM_GloVe                      90.00%\n",
      "  ‚Ä¢ bert_v2                         90.00%\n",
      "\n",
      "üö© Bottom 5 by Accuracy (general):\n",
      "  ‚Ä¢ SVM_TFIDF_(1-3gram)             56.00%\n",
      "  ‚Ä¢ KNN_TFIDF_(1-3gram)             55.00%\n",
      "  ‚Ä¢ LogisticRegression_Bag_of_Words_(1-3gram)  55.00%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)      50.00%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)  50.00%\n",
      "\n",
      "üèÜ Top 5 by Precision (macro-avg):\n",
      "  ‚Ä¢ SVM_GloVe_300d                  95.02%\n",
      "  ‚Ä¢ roberta_v3                      92.07%\n",
      "  ‚Ä¢ roberta_adversarial             91.41%\n",
      "  ‚Ä¢ LSTM_GloVe                      91.05%\n",
      "  ‚Ä¢ LSTM                            91.05%\n",
      "\n",
      "üö© Bottom 5 by Precision (macro-avg):\n",
      "  ‚Ä¢ KNN_Bag_of_Words_(1-3gram)      60.15%\n",
      "  ‚Ä¢ KNN_TFIDF_(1-3gram)             57.06%\n",
      "  ‚Ä¢ KNN_GloVe_300d                  56.37%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)      25.00%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)  25.00%\n",
      "\n",
      "üèÜ Top 5 by Recall (macro-avg):\n",
      "  ‚Ä¢ SVM_GloVe_300d                  95.00%\n",
      "  ‚Ä¢ roberta_v3                      92.00%\n",
      "  ‚Ä¢ roberta_adversarial             91.00%\n",
      "  ‚Ä¢ LSTM_GloVe                      90.00%\n",
      "  ‚Ä¢ bert_v2                         90.00%\n",
      "\n",
      "üö© Bottom 5 by Recall (macro-avg):\n",
      "  ‚Ä¢ SVM_TFIDF_(1-3gram)             56.00%\n",
      "  ‚Ä¢ KNN_TFIDF_(1-3gram)             55.00%\n",
      "  ‚Ä¢ LogisticRegression_Bag_of_Words_(1-3gram)  55.00%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)      50.00%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)  50.00%\n",
      "\n",
      "üèÜ Top 5 by F1 (macro-avg):\n",
      "  ‚Ä¢ SVM_GloVe_300d                  95.00%\n",
      "  ‚Ä¢ roberta_v3                      92.00%\n",
      "  ‚Ä¢ roberta_adversarial             90.98%\n",
      "  ‚Ä¢ bert_v2                         89.98%\n",
      "  ‚Ä¢ LSTM_GloVe                      89.94%\n",
      "\n",
      "üö© Bottom 5 by F1 (macro-avg):\n",
      "  ‚Ä¢ KNN_TFIDF_(1-3gram)             51.46%\n",
      "  ‚Ä¢ SVM_TFIDF_(1-3gram)             46.58%\n",
      "  ‚Ä¢ LogisticRegression_Bag_of_Words_(1-3gram)  44.79%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)      33.33%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)  33.33%\n",
      "\n",
      "üèÜ Top 5 by Precision (FAKE):\n",
      "  ‚Ä¢ LogisticRegression_GloVe_300d  100.00%\n",
      "  ‚Ä¢ roberta_adversarial             95.56%\n",
      "  ‚Ä¢ SVM_GloVe_300d                  94.12%\n",
      "  ‚Ä¢ roberta_v3                      93.75%\n",
      "  ‚Ä¢ LogisticRegression_TFIDF_(1-3gram)  92.31%\n",
      "\n",
      "üö© Bottom 5 by Precision (FAKE):\n",
      "  ‚Ä¢ KNN_Bag_of_Words_(1-3gram)      61.36%\n",
      "  ‚Ä¢ KNN_TFIDF_(1-3gram)             60.87%\n",
      "  ‚Ä¢ KNN_GloVe_300d                  57.89%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)       0.00%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)   0.00%\n",
      "\n",
      "üèÜ Top 5 by Recall (FAKE):\n",
      "  ‚Ä¢ LSTM                            98.00%\n",
      "  ‚Ä¢ LSTM_GloVe                      98.00%\n",
      "  ‚Ä¢ CNN_LSTM                        96.00%\n",
      "  ‚Ä¢ SVM_GloVe_300d                  96.00%\n",
      "  ‚Ä¢ CNN_LSTM_GloVe                  96.00%\n",
      "\n",
      "üö© Bottom 5 by Recall (FAKE):\n",
      "  ‚Ä¢ LogisticRegression_TFIDF_(1-3gram)  24.00%\n",
      "  ‚Ä¢ SVM_TFIDF_(1-3gram)             14.00%\n",
      "  ‚Ä¢ LogisticRegression_Bag_of_Words_(1-3gram)  12.00%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)       0.00%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)   0.00%\n",
      "\n",
      "üèÜ Top 5 by F1 (FAKE):\n",
      "  ‚Ä¢ SVM_GloVe_300d                  95.05%\n",
      "  ‚Ä¢ roberta_v3                      91.84%\n",
      "  ‚Ä¢ LSTM_GloVe                      90.74%\n",
      "  ‚Ä¢ LSTM                            90.74%\n",
      "  ‚Ä¢ roberta_adversarial             90.53%\n",
      "\n",
      "üö© Bottom 5 by F1 (FAKE):\n",
      "  ‚Ä¢ LogisticRegression_TFIDF_(1-3gram)  38.10%\n",
      "  ‚Ä¢ SVM_TFIDF_(1-3gram)             24.14%\n",
      "  ‚Ä¢ LogisticRegression_Bag_of_Words_(1-3gram)  21.05%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)       0.00%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)   0.00%\n",
      "\n",
      "üèÜ Top 5 by Precision (REAL):\n",
      "  ‚Ä¢ LSTM                            97.62%\n",
      "  ‚Ä¢ LSTM_GloVe                      97.62%\n",
      "  ‚Ä¢ SVM_GloVe_300d                  95.92%\n",
      "  ‚Ä¢ CNN_LSTM                        94.44%\n",
      "  ‚Ä¢ CNN_LSTM_GloVe                  94.44%\n",
      "\n",
      "üö© Bottom 5 by Precision (REAL):\n",
      "  ‚Ä¢ SVM_TFIDF_(1-3gram)             53.26%\n",
      "  ‚Ä¢ KNN_TFIDF_(1-3gram)             53.25%\n",
      "  ‚Ä¢ LogisticRegression_Bag_of_Words_(1-3gram)  52.69%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)      50.00%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)  50.00%\n",
      "\n",
      "üèÜ Top 5 by Recall (REAL):\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram) 100.00%\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)     100.00%\n",
      "  ‚Ä¢ LogisticRegression_GloVe_300d  100.00%\n",
      "  ‚Ä¢ LogisticRegression_Bag_of_Words_(1-3gram)  98.00%\n",
      "  ‚Ä¢ SVM_TFIDF_(1-3gram)             98.00%\n",
      "\n",
      "üö© Bottom 5 by Recall (REAL):\n",
      "  ‚Ä¢ CNN_GloVe                       74.00%\n",
      "  ‚Ä¢ CNN_LSTM_GloVe                  68.00%\n",
      "  ‚Ä¢ KNN_GloVe_300d                  68.00%\n",
      "  ‚Ä¢ CNN_LSTM                        68.00%\n",
      "  ‚Ä¢ KNN_Bag_of_Words_(1-3gram)      66.00%\n",
      "\n",
      "üèÜ Top 5 by F1 (REAL):\n",
      "  ‚Ä¢ SVM_GloVe_300d                  94.95%\n",
      "  ‚Ä¢ roberta_v3                      92.16%\n",
      "  ‚Ä¢ roberta_adversarial             91.43%\n",
      "  ‚Ä¢ bert_v2                         89.58%\n",
      "  ‚Ä¢ LogisticRegression_GloVe_300d   89.29%\n",
      "\n",
      "üö© Bottom 5 by F1 (REAL):\n",
      "  ‚Ä¢ NaiveBayes_TFIDF_(1-3gram)      66.67%\n",
      "  ‚Ä¢ NaiveBayes_Bag_of_Words_(1-3gram)  66.67%\n",
      "  ‚Ä¢ KNN_TFIDF_(1-3gram)             64.57%\n",
      "  ‚Ä¢ KNN_Bag_of_Words_(1-3gram)      62.26%\n",
      "  ‚Ä¢ KNN_GloVe_300d                  60.71%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 1) Load merged predictions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "df = pd.read_csv(MERGED_TSV, sep=\"\\t\", dtype=str)\n",
    "\n",
    "# identify model names from the \"_predicted\" columns\n",
    "model_names = [c[:-10] for c in df.columns if c.endswith(\"_predicted\")]\n",
    "\n",
    "# normalize true labels\n",
    "y_true = df[\"label(1=real,0=fake)\"].str.strip().map({\n",
    "    \"1\":\"REAL\",\"__LABEL__1\":\"REAL\",\"REAL\":\"REAL\",\n",
    "    \"0\":\"FAKE\",\"__LABEL__0\":\"FAKE\",\"FAKE\":\"FAKE\"\n",
    "}).tolist()\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 2) Compute all metrics into a DataFrame ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "records = []\n",
    "for m in model_names:\n",
    "    y_pred = df[f\"{m}_predicted\"].tolist()\n",
    "    # general (macro-average)\n",
    "    acc_g   = accuracy_score(y_true, y_pred)\n",
    "    prec_g  = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec_g   = recall_score   (y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_g    = f1_score       (y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    # FAKE as positive\n",
    "    prec_f  = precision_score(y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    rec_f   = recall_score   (y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    f1_f    = f1_score       (y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    # REAL as positive\n",
    "    prec_r  = precision_score(y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "    rec_r   = recall_score   (y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "    f1_r    = f1_score       (y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "\n",
    "    records.append({\n",
    "        \"model\":     m,\n",
    "        \"acc_gen\":   acc_g,\n",
    "        \"prec_gen\":  prec_g,\n",
    "        \"rec_gen\":   rec_g,\n",
    "        \"f1_gen\":    f1_g,\n",
    "        \"prec_fake\": prec_f,\n",
    "        \"rec_fake\":  rec_f,\n",
    "        \"f1_fake\":   f1_f,\n",
    "        \"prec_real\": prec_r,\n",
    "        \"rec_real\":  rec_r,\n",
    "        \"f1_real\":   f1_r,\n",
    "    })\n",
    "\n",
    "dfm = pd.DataFrame(records).set_index(\"model\")\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 3) Define human-friendly titles for each metric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy (general)\":     \"acc_gen\",\n",
    "    \"Precision (macro-avg)\":  \"prec_gen\",\n",
    "    \"Recall (macro-avg)\":     \"rec_gen\",\n",
    "    \"F1 (macro-avg)\":         \"f1_gen\",\n",
    "    \"Precision (FAKE)\":       \"prec_fake\",\n",
    "    \"Recall (FAKE)\":          \"rec_fake\",\n",
    "    \"F1 (FAKE)\":              \"f1_fake\",\n",
    "    \"Precision (REAL)\":       \"prec_real\",\n",
    "    \"Recall (REAL)\":          \"rec_real\",\n",
    "    \"F1 (REAL)\":              \"f1_real\",\n",
    "}\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ 4) For each metric, print Top 5 and Bottom 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "for title, col in metrics.items():\n",
    "    s = dfm[col].sort_values(ascending=False)\n",
    "    print(f\"\\nüèÜ Top 5 by {title}:\")\n",
    "    for model, val in s.head(5).items():\n",
    "        print(f\"  ‚Ä¢ {model:30} {val*100:6.2f}%\")\n",
    "    print(f\"\\nüö© Bottom 5 by {title}:\")\n",
    "    for model, val in s.tail(5).items():\n",
    "        print(f\"  ‚Ä¢ {model:30} {val*100:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc120c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0598d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_DIR = \"../datasets\"\n",
    "\n",
    "# Pattern to match\n",
    "PATTERN = os.path.join(PRED_DIR, \"*_predictions.tsv\")\n",
    "\n",
    "# How many top errors to display\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad81f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found prediction files: ['../datasets/teste100_CNN_LSTM_predictions.tsv', '../datasets/teste100_CNN_predictions.tsv', '../datasets/teste100_KNN_Bag_of_Words_(1-3gram)_predictions.tsv', '../datasets/teste100_KNN_GloVe_300d_predictions.tsv', '../datasets/teste100_KNN_TFIDF_(1-3gram)_predictions.tsv', '../datasets/teste100_LSTM_predictions.tsv', '../datasets/teste100_LogisticRegression_Bag_of_Words_(1-3gram)_predictions.tsv', '../datasets/teste100_LogisticRegression_GloVe_300d_predictions.tsv', '../datasets/teste100_LogisticRegression_TFIDF_(1-3gram)_predictions.tsv', '../datasets/teste100_NaiveBayes_Bag_of_Words_(1-3gram)_predictions.tsv', '../datasets/teste100_NaiveBayes_GloVe_300d_predictions.tsv', '../datasets/teste100_NaiveBayes_TFIDF_(1-3gram)_predictions.tsv', '../datasets/teste100_RandomForest_Bag_of_Words_(1-3gram)_predictions.tsv', '../datasets/teste100_RandomForest_GloVe_300d_predictions.tsv', '../datasets/teste100_RandomForest_TFIDF_(1-3gram)_predictions.tsv', '../datasets/teste100_SVM_Bag_of_Words_(1-3gram)_predictions.tsv', '../datasets/teste100_SVM_GloVe_300d_predictions.tsv', '../datasets/teste100_SVM_TFIDF_(1-3gram)_predictions.tsv', '../datasets/teste100_bert_v2_predictions.tsv', '../datasets/teste100_fasttext_cnn_lstm_predictions.tsv', '../datasets/teste100_roberta_v3_predictions.tsv', '../datasets/teste100_with_predictions.tsv']\n",
      "  ‚Üí teste100_CNN_LSTM_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_CNN_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_KNN_Bag_of_Words_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_KNN_GloVe_300d_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_KNN_TFIDF_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_LSTM_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_LogisticRegression_Bag_of_Words_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_LogisticRegression_GloVe_300d_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_LogisticRegression_TFIDF_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_NaiveBayes_Bag_of_Words_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_NaiveBayes_GloVe_300d_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_NaiveBayes_TFIDF_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_RandomForest_Bag_of_Words_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_RandomForest_GloVe_300d_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_RandomForest_TFIDF_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_SVM_Bag_of_Words_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_SVM_GloVe_300d_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_SVM_TFIDF_(1-3gram)_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_bert_v2_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_fasttext_cnn_lstm_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_roberta_v3_predictions.tsv: 100 rows before cleaning\n",
      "  ‚Üí teste100_with_predictions.tsv: 100 rows before cleaning\n",
      "\n",
      "After concatenation: 100 rows total\n",
      "Dropped 0 rows because true_label was NaN (now 100 rows)\n",
      "\n",
      "Unique true_label values: {'FAKE': 50, 'REAL': 50}\n"
     ]
    }
   ],
   "source": [
    "files = sorted(glob.glob(PATTERN))\n",
    "print(\"Found prediction files:\", files)\n",
    "\n",
    "dfs = []\n",
    "model_names = []\n",
    "for path in files:\n",
    "    df = pd.read_csv(path, sep=\"\\t\", dtype={\"label(1=real,0=fake)\": str})\n",
    "    print(f\"  ‚Üí {os.path.basename(path)}: {len(df)} rows before cleaning\")\n",
    "\n",
    "    # count any missing text\n",
    "    n_missing = df[\"text\"].isna().sum()\n",
    "    if n_missing:\n",
    "        print(f\"    ‚ö†Ô∏è  {n_missing} rows with missing text\")\n",
    "\n",
    "    basename = os.path.basename(path)\n",
    "    model = basename.replace(\"_predictions.tsv\",\"\")\n",
    "    model_names.append(model)\n",
    "    df = df.rename(columns={\n",
    "        \"predicted\":      f\"pred_{model}\",\n",
    "        \"predicted_prob\": f\"prob_{model}\",\n",
    "        \"label(1=real,0=fake)\": \"true_raw\"\n",
    "    })\n",
    "    dfs.append(df[[\"text\", \"true_raw\", f\"pred_{model}\"]])\n",
    "\n",
    "# merge\n",
    "merged = dfs[0][[\"text\", \"true_raw\"]].copy()\n",
    "for df in dfs:\n",
    "    merged = pd.concat([merged, df.drop(columns=[\"text\",\"true_raw\"])], axis=1)\n",
    "\n",
    "print(f\"\\nAfter concatenation: {len(merged)} rows total\")\n",
    "\n",
    "# normalize and drop\n",
    "merged[\"true_label\"] = merged[\"true_raw\"].str.strip().map({\n",
    "    \"1\":\"REAL\",\"__LABEL__1\":\"REAL\",\"REAL\":\"REAL\",\n",
    "    \"0\":\"FAKE\",\"__LABEL__0\":\"FAKE\",\"FAKE\":\"FAKE\"\n",
    "})\n",
    "before_drop = len(merged)\n",
    "merged = merged.dropna(subset=[\"true_label\"])\n",
    "after_drop = len(merged)\n",
    "print(f\"Dropped {before_drop-after_drop} rows because true_label was NaN (now {after_drop} rows)\\n\")\n",
    "\n",
    "# sanity check\n",
    "print(\"Unique true_label values:\", merged[\"true_label\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e3051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Fake‚Äênews detection accuracy (best ‚Üí worst):\n",
      "  teste100_LSTM                            98.00%\n",
      "  teste100_CNN_LSTM                        96.00%\n",
      "  teste100_SVM_GloVe_300d                  96.00%\n",
      "  teste100_CNN                             94.00%\n",
      "  teste100_bert_v2                         94.00%\n",
      "  teste100_roberta_v3                      90.00%\n",
      "  teste100_with                            86.00%\n",
      "  teste100_fasttext_cnn_lstm               82.00%\n",
      "  teste100_LogisticRegression_GloVe_300d   76.00%\n",
      "  teste100_RandomForest_GloVe_300d         76.00%\n",
      "  teste100_RandomForest_TFIDF_(1-3gram)    76.00%\n",
      "  teste100_RandomForest_Bag_of_Words_(1-3gram) 70.00%\n",
      "  teste100_NaiveBayes_GloVe_300d           58.00%\n",
      "  teste100_KNN_Bag_of_Words_(1-3gram)      54.00%\n",
      "  teste100_SVM_Bag_of_Words_(1-3gram)      50.00%\n",
      "  teste100_KNN_GloVe_300d                  44.00%\n",
      "  teste100_KNN_TFIDF_(1-3gram)             28.00%\n",
      "  teste100_LogisticRegression_TFIDF_(1-3gram) 24.00%\n",
      "  teste100_SVM_TFIDF_(1-3gram)             14.00%\n",
      "  teste100_LogisticRegression_Bag_of_Words_(1-3gram) 12.00%\n",
      "  teste100_NaiveBayes_Bag_of_Words_(1-3gram)  0.00%\n",
      "  teste100_NaiveBayes_TFIDF_(1-3gram)       0.00%\n"
     ]
    }
   ],
   "source": [
    "#  ‚Äî‚Äî‚Äî  rank by fake‚Äênews accuracy  ‚Äî‚Äî‚Äî\n",
    "\n",
    "# mask for all the true‚Äêfake examples\n",
    "fake_mask = merged[\"true_label\"] == \"FAKE\"\n",
    "total_fakes = fake_mask.sum()\n",
    "\n",
    "fake_accs = {}\n",
    "for m in model_names:\n",
    "    # count how many of the FAKEs each model got right\n",
    "    correct = (merged.loc[fake_mask, f\"pred_{m}\"] == \"FAKE\").sum()\n",
    "    fake_accs[m] = correct / total_fakes\n",
    "\n",
    "# sort descending\n",
    "ranked = sorted(fake_accs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüìä Fake‚Äênews detection accuracy (best ‚Üí worst):\")\n",
    "for name, acc in ranked:\n",
    "    print(f\"  {name:<40} {acc*100:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfcaa439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Top-10 by Accuracy (general)\n",
      "| model                                  |   Accuracy (general) |\n",
      "|:---------------------------------------|---------------------:|\n",
      "| teste100_SVM_GloVe_300d                |                 0.95 |\n",
      "| teste100_roberta_v3                    |                 0.92 |\n",
      "| teste100_with                          |                 0.91 |\n",
      "| teste100_bert_v2                       |                 0.9  |\n",
      "| teste100_LSTM                          |                 0.9  |\n",
      "| teste100_LogisticRegression_GloVe_300d |                 0.88 |\n",
      "| teste100_CNN                           |                 0.84 |\n",
      "| teste100_RandomForest_GloVe_300d       |                 0.83 |\n",
      "| teste100_CNN_LSTM                      |                 0.82 |\n",
      "| teste100_RandomForest_TFIDF_(1-3gram)  |                 0.81 |\n",
      "\n",
      "üèÜ Top-10 by Precision (macro)\n",
      "| model                                  |   Precision (macro) |\n",
      "|:---------------------------------------|--------------------:|\n",
      "| teste100_SVM_GloVe_300d                |            0.95018  |\n",
      "| teste100_roberta_v3                    |            0.920673 |\n",
      "| teste100_with                          |            0.914141 |\n",
      "| teste100_LSTM                          |            0.910509 |\n",
      "| teste100_LogisticRegression_GloVe_300d |            0.903226 |\n",
      "| teste100_bert_v2                       |            0.902576 |\n",
      "| teste100_CNN                           |            0.854167 |\n",
      "| teste100_CNN_LSTM                      |            0.847222 |\n",
      "| teste100_RandomForest_GloVe_300d       |            0.836597 |\n",
      "| teste100_RandomForest_TFIDF_(1-3gram)  |            0.813131 |\n",
      "\n",
      "üèÜ Top-10 by Recall (macro)\n",
      "| model                                  |   Recall (macro) |\n",
      "|:---------------------------------------|-----------------:|\n",
      "| teste100_SVM_GloVe_300d                |             0.95 |\n",
      "| teste100_roberta_v3                    |             0.92 |\n",
      "| teste100_with                          |             0.91 |\n",
      "| teste100_bert_v2                       |             0.9  |\n",
      "| teste100_LSTM                          |             0.9  |\n",
      "| teste100_LogisticRegression_GloVe_300d |             0.88 |\n",
      "| teste100_CNN                           |             0.84 |\n",
      "| teste100_RandomForest_GloVe_300d       |             0.83 |\n",
      "| teste100_CNN_LSTM                      |             0.82 |\n",
      "| teste100_RandomForest_TFIDF_(1-3gram)  |             0.81 |\n",
      "\n",
      "üèÜ Top-10 by F1 (macro)\n",
      "| model                                  |   F1 (macro) |\n",
      "|:---------------------------------------|-------------:|\n",
      "| teste100_SVM_GloVe_300d                |     0.949995 |\n",
      "| teste100_roberta_v3                    |     0.919968 |\n",
      "| teste100_with                          |     0.909774 |\n",
      "| teste100_bert_v2                       |     0.89984  |\n",
      "| teste100_LSTM                          |     0.899356 |\n",
      "| teste100_LogisticRegression_GloVe_300d |     0.878247 |\n",
      "| teste100_CNN                           |     0.838384 |\n",
      "| teste100_RandomForest_GloVe_300d       |     0.829163 |\n",
      "| teste100_CNN_LSTM                      |     0.816401 |\n",
      "| teste100_RandomForest_TFIDF_(1-3gram)  |     0.809524 |\n",
      "\n",
      "üèÜ Top-10 by Precision (FAKE)\n",
      "| model                                              |   Precision (FAKE) |\n",
      "|:---------------------------------------------------|-------------------:|\n",
      "| teste100_LogisticRegression_GloVe_300d             |           1        |\n",
      "| teste100_with                                      |           0.955556 |\n",
      "| teste100_SVM_GloVe_300d                            |           0.941176 |\n",
      "| teste100_roberta_v3                                |           0.9375   |\n",
      "| teste100_LogisticRegression_TFIDF_(1-3gram)        |           0.923077 |\n",
      "| teste100_SVM_Bag_of_Words_(1-3gram)                |           0.892857 |\n",
      "| teste100_RandomForest_GloVe_300d                   |           0.883721 |\n",
      "| teste100_SVM_TFIDF_(1-3gram)                       |           0.875    |\n",
      "| teste100_bert_v2                                   |           0.87037  |\n",
      "| teste100_LogisticRegression_Bag_of_Words_(1-3gram) |           0.857143 |\n",
      "\n",
      "üèÜ Top-10 by Recall (FAKE)\n",
      "| model                                  |   Recall (FAKE) |\n",
      "|:---------------------------------------|----------------:|\n",
      "| teste100_LSTM                          |            0.98 |\n",
      "| teste100_CNN_LSTM                      |            0.96 |\n",
      "| teste100_SVM_GloVe_300d                |            0.96 |\n",
      "| teste100_CNN                           |            0.94 |\n",
      "| teste100_bert_v2                       |            0.94 |\n",
      "| teste100_roberta_v3                    |            0.9  |\n",
      "| teste100_with                          |            0.86 |\n",
      "| teste100_fasttext_cnn_lstm             |            0.82 |\n",
      "| teste100_LogisticRegression_GloVe_300d |            0.76 |\n",
      "| teste100_RandomForest_GloVe_300d       |            0.76 |\n",
      "\n",
      "üèÜ Top-10 by F1 (FAKE)\n",
      "| model                                  |   F1 (FAKE) |\n",
      "|:---------------------------------------|------------:|\n",
      "| teste100_SVM_GloVe_300d                |    0.950495 |\n",
      "| teste100_roberta_v3                    |    0.918367 |\n",
      "| teste100_LSTM                          |    0.907407 |\n",
      "| teste100_with                          |    0.905263 |\n",
      "| teste100_bert_v2                       |    0.903846 |\n",
      "| teste100_LogisticRegression_GloVe_300d |    0.863636 |\n",
      "| teste100_CNN                           |    0.854545 |\n",
      "| teste100_CNN_LSTM                      |    0.842105 |\n",
      "| teste100_RandomForest_GloVe_300d       |    0.817204 |\n",
      "| teste100_RandomForest_TFIDF_(1-3gram)  |    0.8      |\n",
      "\n",
      "üèÜ Top-10 by Precision (REAL)\n",
      "| model                                  |   Precision (REAL) |\n",
      "|:---------------------------------------|-------------------:|\n",
      "| teste100_LSTM                          |           0.97619  |\n",
      "| teste100_SVM_GloVe_300d                |           0.959184 |\n",
      "| teste100_CNN_LSTM                      |           0.944444 |\n",
      "| teste100_bert_v2                       |           0.934783 |\n",
      "| teste100_CNN                           |           0.925    |\n",
      "| teste100_roberta_v3                    |           0.903846 |\n",
      "| teste100_with                          |           0.872727 |\n",
      "| teste100_LogisticRegression_GloVe_300d |           0.806452 |\n",
      "| teste100_RandomForest_GloVe_300d       |           0.789474 |\n",
      "| teste100_RandomForest_TFIDF_(1-3gram)  |           0.781818 |\n",
      "\n",
      "üèÜ Top-10 by Recall (REAL)\n",
      "| model                                              |   Recall (REAL) |\n",
      "|:---------------------------------------------------|----------------:|\n",
      "| teste100_NaiveBayes_TFIDF_(1-3gram)                |            1    |\n",
      "| teste100_NaiveBayes_Bag_of_Words_(1-3gram)         |            1    |\n",
      "| teste100_LogisticRegression_GloVe_300d             |            1    |\n",
      "| teste100_LogisticRegression_Bag_of_Words_(1-3gram) |            0.98 |\n",
      "| teste100_SVM_TFIDF_(1-3gram)                       |            0.98 |\n",
      "| teste100_LogisticRegression_TFIDF_(1-3gram)        |            0.98 |\n",
      "| teste100_with                                      |            0.96 |\n",
      "| teste100_SVM_Bag_of_Words_(1-3gram)                |            0.94 |\n",
      "| teste100_SVM_GloVe_300d                            |            0.94 |\n",
      "| teste100_roberta_v3                                |            0.94 |\n",
      "\n",
      "üèÜ Top-10 by F1 (REAL)\n",
      "| model                                        |   F1 (REAL) |\n",
      "|:---------------------------------------------|------------:|\n",
      "| teste100_SVM_GloVe_300d                      |    0.949495 |\n",
      "| teste100_roberta_v3                          |    0.921569 |\n",
      "| teste100_with                                |    0.914286 |\n",
      "| teste100_bert_v2                             |    0.895833 |\n",
      "| teste100_LogisticRegression_GloVe_300d       |    0.892857 |\n",
      "| teste100_LSTM                                |    0.891304 |\n",
      "| teste100_RandomForest_GloVe_300d             |    0.841121 |\n",
      "| teste100_CNN                                 |    0.822222 |\n",
      "| teste100_RandomForest_TFIDF_(1-3gram)        |    0.819048 |\n",
      "| teste100_RandomForest_Bag_of_Words_(1-3gram) |    0.796296 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Build a small metrics table: rows=models, cols=metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "metrics = []\n",
    "y_true = merged[\"true_label\"].tolist()\n",
    "\n",
    "for m in model_names:\n",
    "    y_pred = merged[f\"pred_{m}\"].tolist()\n",
    "    # general\n",
    "    acc_gen = accuracy_score(y_true, y_pred)\n",
    "    prec_gen = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec_gen  = recall_score   (y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_gen   = f1_score       (y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    # fake as positive\n",
    "    prec_fake = precision_score(y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    rec_fake  = recall_score   (y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    f1_fake   = f1_score       (y_true, y_pred, pos_label=\"FAKE\", zero_division=0)\n",
    "    # real as positive\n",
    "    prec_real = precision_score(y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "    rec_real  = recall_score   (y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "    f1_real   = f1_score       (y_true, y_pred, pos_label=\"REAL\", zero_division=0)\n",
    "\n",
    "    metrics.append({\n",
    "        \"model\":      m,\n",
    "        # general\n",
    "        \"acc_gen\":    acc_gen,\n",
    "        \"prec_gen\":   prec_gen,\n",
    "        \"rec_gen\":    rec_gen,\n",
    "        \"f1_gen\":     f1_gen,\n",
    "        # fake\n",
    "        \"prec_fake\":  prec_fake,\n",
    "        \"rec_fake\":   rec_fake,\n",
    "        \"f1_fake\":    f1_fake,\n",
    "        # real\n",
    "        \"prec_real\":  prec_real,\n",
    "        \"rec_real\":   rec_real,\n",
    "        \"f1_real\":    f1_real,\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "dfm = pd.DataFrame(metrics).set_index(\"model\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) A helper to print Top-10 for any column\n",
    "# -----------------------------------------------------------------------------\n",
    "def print_top(col, title):\n",
    "    print(f\"\\nüèÜ Top-10 by {title}\")\n",
    "    top = dfm[col].sort_values(ascending=False).head(10)\n",
    "    # pretty-print\n",
    "    print(top.to_frame(name=title).to_markdown())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Print all Twelve lists\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# General\n",
    "print_top(\"acc_gen\",   \"Accuracy (general)\")\n",
    "print_top(\"prec_gen\",  \"Precision (macro)\")\n",
    "print_top(\"rec_gen\",   \"Recall (macro)\")\n",
    "print_top(\"f1_gen\",    \"F1 (macro)\")\n",
    "\n",
    "# Fake class\n",
    "print_top(\"prec_fake\", \"Precision (FAKE)\")\n",
    "print_top(\"rec_fake\",  \"Recall (FAKE)\")\n",
    "print_top(\"f1_fake\",   \"F1 (FAKE)\")\n",
    "\n",
    "# Real class\n",
    "print_top(\"prec_real\", \"Precision (REAL)\")\n",
    "print_top(\"rec_real\",  \"Recall (REAL)\")\n",
    "print_top(\"f1_real\",   \"F1 (REAL)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d93567b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FAKE',\n",
       " 'REAL',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'REAL',\n",
       " 'FAKE',\n",
       " 'REAL',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE',\n",
       " 'FAKE']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcf6a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
