{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlad.cristescu/miniconda3/envs/proiect_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-06 15:11:15.941835: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemperatureScaler, self).__init__()\n",
    "        # log_temperature pentru a ne asigura că T > 0 (T = exp(log_temperature))\n",
    "        self.log_temperature = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, logits):\n",
    "        temperature = torch.exp(self.log_temperature)\n",
    "        return logits / temperature\n",
    "\n",
    "class CalibratedModel(nn.Module):\n",
    "    def __init__(self, model, temp_scaler):\n",
    "        super(CalibratedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.temp_scaler = temp_scaler\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        scaled_logits = self.temp_scaler(logits)\n",
    "        calibrated_probs = F.softmax(scaled_logits, dim=-1)\n",
    "        return calibrated_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"saved_models/roberta/roberta_torch_model\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"saved_models/roberta/roberta_tokenizer\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedModel(\n",
       "  (model): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (temp_scaler): TemperatureScaler()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_scaler = TemperatureScaler()\n",
    "calibrated_model = CalibratedModel(model, temp_scaler)\n",
    "calibrated_model.load_state_dict(torch.load(\"saved_models/roberta/calibrated_model.pth\", map_location=\"cpu\"))\n",
    "calibrated_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def wordopt(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The avalanche of fake news, pro-Putin and anti-European messages on social media puts Romanian authorities on alert. SRI teaches people how to spot online dangers, MIA reports that eleven fake accounts are disinformation in the name of the Police. And the CNA found illegal posts on TikTok and Facebook and asked the two platforms to remove them.\n",
      "Probabilități calibrate: [0.025622986257076263, 0.9743770360946655]\n",
      "Eticheta prezisă (calibrată): REAL\n"
     ]
    }
   ],
   "source": [
    "text_example = '''The avalanche of fake news, pro-Putin and anti-European messages on social media puts Romanian authorities on alert. SRI teaches people how to spot online dangers, MIA reports that eleven fake accounts are disinformation in the name of the Police. And the CNA found illegal posts on TikTok and Facebook and asked the two platforms to remove them.'''\n",
    "\n",
    "text_example = wordopt(text_example)\n",
    "encoded_input = tokenizer.encode_plus(\n",
    "    text_example,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512\n",
    ")\n",
    "with torch.no_grad():\n",
    "    calibrated_probs = calibrated_model(\n",
    "        input_ids=encoded_input[\"input_ids\"],\n",
    "        attention_mask=encoded_input[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "pred_label = torch.argmax(calibrated_probs, dim=-1).item()\n",
    "\n",
    "print(\"Text:\", text_example)\n",
    "print(\"Probabilități calibrate:\", calibrated_probs.squeeze().tolist())\n",
    "decizie = \"REAL\" if pred_label == 1 else \"FAKE\"\n",
    "print(\"Eticheta prezisă (calibrată):\", decizie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The Pardons that Sleepy Joe Biden gave to the Unselect Committee of Political Thugs, and many others, are hereby declared VOID, VACANT, AND OF NO FURTHER FORCE OR EFFECT, because of the fact that they were done by Autopen. In other words, Joe Biden did not sign them but, more importantly, he did not know anything about them! The necessary Pardoning Documents were not explained to, or approved by, Biden. He knew nothing about them, and the people that did may have committed a crime. Therefore, those on the Unselect Committee, who destroyed and deleted ALL evidence obtained during their two year Witch Hunt of me, and many other innocent people, should fully understand that they are subject to investigation at the highest level. The fact is, they were probably responsible for the Documents that were signed on their behalf without the knowledge or consent of the Worst President in the History of our Country, Crooked Joe Biden!\n",
      "Probabilități necalibrate: [0.9997610449790955, 0.00023895646154414862]\n",
      "Eticheta prezisă (necalibrat): 0\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer.encode_plus(\n",
    "    text_example,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "    logits = outputs.logits\n",
    "    # Softmax direct pe logits = probabilități necalibrate\n",
    "    probs_uncalibrated = torch.softmax(logits, dim=-1)\n",
    "    pred_label_uncalibrated = torch.argmax(probs_uncalibrated, dim=-1).item()\n",
    "\n",
    "print(\"Text:\", text_example)\n",
    "print(\"Probabilități necalibrate:\", probs_uncalibrated.squeeze().tolist())\n",
    "print(\"Eticheta prezisă (necalibrat):\", pred_label_uncalibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valoare temperatură: 1.1422650814056396\n"
     ]
    }
   ],
   "source": [
    "temperature_value = torch.exp(temp_scaler.log_temperature).item()\n",
    "print(\"Valoare temperatură:\", temperature_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Încarcă modelul calibrat\u001b[39;00m\n\u001b[1;32m     28\u001b[0m temp_scaler \u001b[38;5;241m=\u001b[39m TemperatureScaler()\n\u001b[0;32m---> 29\u001b[0m calibrated_model \u001b[38;5;241m=\u001b[39m CalibratedModel(\u001b[43mmodel\u001b[49m, temp_scaler)\n\u001b[1;32m     30\u001b[0m calibrated_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/roberta/calibrated_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     31\u001b[0m calibrated_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dacă ai și un model calibrat (wrapper-ul CalibratedModel) salvat, încarcă-l.\n",
    "# Pentru acest exemplu, presupunem că ai salvat modelul calibrat ca \"calibrated_model.pth\"\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"../../datasets/WELFake_cleaned.csv\")\n",
    "\n",
    "class TemperatureScaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TemperatureScaler, self).__init__()\n",
    "        self.log_temperature = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, logits):\n",
    "        temperature = torch.exp(self.log_temperature)\n",
    "        return logits / temperature\n",
    "\n",
    "class CalibratedModel(nn.Module):\n",
    "    def __init__(self, model, temp_scaler):\n",
    "        super(CalibratedModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.temp_scaler = temp_scaler\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        scaled_logits = self.temp_scaler(logits)\n",
    "        return F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "# Încarcă modelul calibrat\n",
    "temp_scaler = TemperatureScaler()\n",
    "calibrated_model = CalibratedModel(model, temp_scaler)\n",
    "calibrated_model.load_state_dict(torch.load(\"saved_models/roberta/calibrated_model.pth\", map_location=\"cpu\"))\n",
    "calibrated_model.eval()\n",
    "\n",
    "# Funcție pentru a obține predicții dintr-o listă de texte (fără DataLoader)\n",
    "def get_predictions_from_texts(texts, model, tokenizer, batch_size=16):\n",
    "    all_probs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i: i+batch_size]\n",
    "        encoded = tokenizer(batch_texts, return_tensors=\"pt\", \n",
    "                            padding=\"max_length\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            probs = F.softmax(outputs.logits, dim=-1)\n",
    "            all_probs.append(probs)\n",
    "    all_probs = torch.cat(all_probs, dim=0).cpu().numpy()\n",
    "    return all_probs\n",
    "\n",
    "def get_predictions_from_texts_calibrated(texts, calibrated_model, tokenizer, batch_size=16):\n",
    "    all_probs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i: i+batch_size]\n",
    "        encoded = tokenizer(batch_texts, return_tensors=\"pt\", \n",
    "                            padding=\"max_length\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            probs = calibrated_model(input_ids=encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            all_probs.append(probs)\n",
    "    all_probs = torch.cat(all_probs, dim=0).cpu().numpy()\n",
    "    return all_probs\n",
    "\n",
    "# Funcție pentru calculul ECE și MCE\n",
    "def compute_calibration_metrics(probs, labels, n_bins=10):\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = (predictions == np.array(labels)).astype(np.float32)\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    mce = 0.0\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences >= bin_boundaries[i]) & (confidences < bin_boundaries[i+1])\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            gap = abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "            ece += gap * prop_in_bin\n",
    "            mce = max(mce, gap)\n",
    "    return ece, mce\n",
    "\n",
    "# Obține predicțiile pentru modelul necalibrat\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "probs_uncalibrated = get_predictions_from_texts(texts, model, tokenizer, batch_size=2)\n",
    "ece_uncalibrated, mce_uncalibrated = compute_calibration_metrics(probs_uncalibrated, labels, n_bins=10)\n",
    "print(\"Model necalibrat:\")\n",
    "print(f\"ECE: {ece_uncalibrated:.4f}\")\n",
    "print(f\"MCE: {mce_uncalibrated:.4f}\")\n",
    "\n",
    "# Obține predicțiile pentru modelul calibrat\n",
    "probs_calibrated = get_predictions_from_texts_calibrated(texts, calibrated_model, tokenizer, batch_size=2)\n",
    "ece_calibrated, mce_calibrated = compute_calibration_metrics(probs_calibrated, labels, n_bins=10)\n",
    "print(\"\\nModel calibrat:\")\n",
    "print(f\"ECE: {ece_calibrated:.4f}\")\n",
    "print(f\"MCE: {mce_calibrated:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proiect_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
