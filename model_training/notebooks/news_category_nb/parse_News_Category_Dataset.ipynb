{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take news content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File ../datasets/News_Category_Dataset_v3.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../datasets/News_Category_Dataset_v3.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/proiect_env/lib/python3.9/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/miniconda3/envs/proiect_env/lib/python3.9/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/proiect_env/lib/python3.9/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File ../datasets/News_Category_Dataset_v3.json does not exist"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('../datasets/News_Category_Dataset_v3.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209527, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reluăm procesarea de la index: 1000 din 209527 articole.\n",
      "Processing article 1100: https://www.huffpost.com/entry/capitol-insurrectio...\n",
      "Processing article 1200: https://www.huffpost.com/entry/price-is-right-wron...\n",
      "Processing article 1300: https://www.huffpost.com/entry/josh-hawley-democra...\n",
      "Processing article 1400: https://www.huffpost.com/entry/betty-white-dead-di...\n",
      "Processing article 1500: https://www.huffpost.com/entry/cris-collinsworth-a...\n",
      "Processing article 1600: https://www.huffpost.com/entry/ap-us-hoffa-search-...\n",
      "Processing article 1700: https://www.huffpost.com/entry/tension-rising-iraq...\n",
      "Processing article 1800: https://www.huffpost.com/entry/best-halloween-cock...\n",
      "Processing article 1900: https://www.huffpost.com/entry/bomb-kabul-mosque-k...\n",
      "Processing article 2000: https://www.huffpost.com/entry/san-francisco-train...\n",
      "Checkpoint: 2000 articole procesate. Salvare completă pentru acest batch.\n",
      "Processing article 2100: https://www.huffpost.com/entry/prisons-and-jails-n...\n",
      "Processing article 2200: https://www.huffpost.com/entry/keith-poe-hand-feed...\n",
      "Processing article 2300: https://www.huffpost.com/entry/maria-taylor-nbc-es...\n",
      "Processing article 2400: https://www.huffpost.com/entry/marjorie-taylor-gre...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import csv\n",
    "from newspaper import Article\n",
    "\n",
    "def process_article(idx, url, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Funcție care procesează un articol dat indexul și URL-ul.\n",
    "    Returnează o tuplă (idx, text), unde text este conținutul extras\n",
    "    sau un mesaj de eroare.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url, language=language)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        text = article.text\n",
    "    except Exception as e:\n",
    "        text = f\"Eroare: {e}\"\n",
    "    return idx, text\n",
    "\n",
    "def load_resume_index(last_index_file):\n",
    "    try:\n",
    "        with open(last_index_file, \"r\") as f:\n",
    "            return int(f.read().strip())\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def save_resume_index(last_index_file, index):\n",
    "    with open(last_index_file, \"w\") as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "def main():\n",
    "    input_json = \"../../datasets/News_Category_Dataset_v3.json\"  \n",
    "    output_csv = \"news_articles_content_full.csv\"             \n",
    "    last_index_file = \"last_index.txt\"                         \n",
    "\n",
    "    df = pd.read_json(input_json, lines=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    total_articles = len(df)\n",
    "    resume_index = load_resume_index(last_index_file)\n",
    "    print(f\"Reluăm procesarea de la index: {resume_index} din {total_articles} articole.\")\n",
    "\n",
    "    if resume_index == 0:\n",
    "        mode = \"w\"\n",
    "    else:\n",
    "        mode = \"a\"\n",
    "    \n",
    "    checkpoint = 1000         \n",
    "    progress_interval = 100   \n",
    "    max_workers = 30           \n",
    "    \n",
    "    with open(output_csv, mode, encoding=\"utf-8\", newline=\"\") as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        if resume_index == 0:\n",
    "            writer.writerow([\"category\", \"content\"])\n",
    "        \n",
    "        for batch_start in range(resume_index, total_articles, checkpoint):\n",
    "            batch_end = min(batch_start + checkpoint, total_articles)\n",
    "            batch_indices = list(range(batch_start, batch_end))\n",
    "            \n",
    "            tasks = [(idx, df.at[idx, \"link\"]) for idx in batch_indices]\n",
    "            results = []\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                \n",
    "                future_to_idx = {executor.submit(process_article, idx, url): idx for idx, url in tasks}\n",
    "                for future in concurrent.futures.as_completed(future_to_idx):\n",
    "                    idx_processed, text = future.result()\n",
    "                    results.append((idx_processed, text))\n",
    "                    \n",
    "                    if (idx_processed + 1) % progress_interval == 0:\n",
    "                        print(f\"Processing article {idx_processed + 1}: {df.at[idx_processed, 'link'][:50]}...\")\n",
    "            \n",
    "            results.sort(key=lambda x: x[0])\n",
    "            \n",
    "            for idx_processed, text in results:\n",
    "                category = df.at[idx_processed, \"category\"]\n",
    "                category_esc = category.replace('\"', '\"\"')\n",
    "                text_esc = text.replace('\"', '\"\"')\n",
    "                writer.writerow([category_esc, text_esc])\n",
    "            \n",
    "            fout.flush()\n",
    "            save_resume_index(last_index_file, batch_end)\n",
    "            print(f\"Checkpoint: {batch_end} articole procesate. Salvare completă pentru acest batch.\")\n",
    "            time.sleep(0.3)  \n",
    "\n",
    "    save_resume_index(last_index_file, total_articles)\n",
    "    print(f\"Procesare completă! Articole totale: {total_articles}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reluăm procesarea de la index: 207901 din 209527 articole.\n",
      "Processing article 208000: https://www.huffingtonpost.com/entry/clothes-washe...\n",
      "Processing article 208100: https://www.huffingtonpost.com/entry/alexander-wan...\n",
      "Processing article 208200: https://www.huffingtonpost.com/entry/sitting-healt...\n",
      "Processing article 208300: https://www.huffingtonpost.com/entry/chloe-moretz-...\n",
      "Processing article 208400: https://www.huffingtonpost.comhttp://online.wsj.co...\n",
      "Processing article 208500: https://www.huffingtonpost.com/entry/molly-sims-ba...\n",
      "Processing article 208600: https://www.huffingtonpost.com/entry/elizabeth-wei...\n",
      "Processing article 208700: https://www.huffingtonpost.comhttp://www.realsimpl...\n",
      "Processing article 208800: https://www.huffingtonpost.com/entry/know-thine-en...\n",
      "Processing article 208900: https://www.huffingtonpost.com/entry/mens-accessor...\n",
      "Checkpoint: 208901 articole procesate. Salvare completă pentru acest batch.\n",
      "Processing article 209000: https://www.huffingtonpost.com/entry/emma-watson-s...\n",
      "Processing article 209100: https://www.huffingtonpost.comhttp://beckermanbite...\n",
      "Processing article 209200: https://www.huffingtonpost.com/entry/karl-net-a-po...\n",
      "Processing article 209300: https://www.huffingtonpost.com/entry/water-parks-i...\n",
      "Processing article 209400: https://www.huffingtonpost.com/entry/location-itch...\n",
      "Processing article 209500: https://www.huffingtonpost.com/entry/hands-on-lear...\n",
      "Checkpoint: 209527 articole procesate. Salvare completă pentru acest batch.\n",
      "Procesare completă! Articole totale: 209527\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_article_bs(url, timeout=10, max_retries=5):\n",
    "   \n",
    "    retries = 0\n",
    "    delay = 1  \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            r = requests.get(url, timeout=timeout)\n",
    "            if r.status_code == 429:\n",
    "                raise Exception(\"HTTP 429\")\n",
    "            if r.status_code != 200:\n",
    "                return f\"Eroare: HTTP {r.status_code}\"\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            for tag in soup([\"script\", \"style\", \"aside\", \"nav\"]):\n",
    "                tag.decompose()\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            text_parts = []\n",
    "            for p in paragraphs:\n",
    "                text = p.get_text(strip=True)\n",
    "                if \"Advertisement\" in text or \"LOADING ERROR LOADING\" in text:\n",
    "                    continue\n",
    "                if text:\n",
    "                    text_parts.append(text)\n",
    "            article_text = \" \".join(text_parts)\n",
    "            return article_text\n",
    "        except Exception as e:\n",
    "            if \"HTTP 429\" in str(e):\n",
    "                print(f\"HTTP 429 encountered at {url}. Waiting for {delay} seconds before retrying...\")\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "                delay *= 2  \n",
    "            else:\n",
    "                return f\"Eroare: {e}\"\n",
    "    return \"Eroare: Max retries atins (HTTP 429)\"\n",
    "\n",
    "def process_article_bs(idx, url):\n",
    "    \n",
    "    text = scrape_article_bs(url)\n",
    "    return idx, text\n",
    "\n",
    "def load_resume_index(last_index_file):\n",
    "    try:\n",
    "        with open(last_index_file, \"r\") as f:\n",
    "            return int(f.read().strip())\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def save_resume_index(last_index_file, index):\n",
    "    with open(last_index_file, \"w\") as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "def main():\n",
    "    input_json = \"../../datasets/News_Category_Dataset_v3.json\" \n",
    "    output_csv = \"news_articles_content_full_bs.csv\"          \n",
    "    last_index_file = \"last_index_bs.txt\"                       \n",
    "    \n",
    "    df = pd.read_json(input_json, lines=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "    total_articles = len(df)\n",
    "    \n",
    "    resume_index = load_resume_index(last_index_file)\n",
    "    print(f\"Reluăm procesarea de la index: {resume_index} din {total_articles} articole.\")\n",
    "    \n",
    "    if resume_index == 0:\n",
    "        mode = \"w\"\n",
    "    else:\n",
    "        mode = \"a\"\n",
    "    \n",
    "    checkpoint = 1000         \n",
    "    progress_interval = 100   \n",
    "    max_workers = 2          \n",
    "    \n",
    "    with open(output_csv, mode, encoding=\"utf-8\", newline=\"\") as fout:\n",
    "        writer = csv.writer(fout)\n",
    "        if resume_index == 0:\n",
    "            writer.writerow([\"category\", \"content\"])\n",
    "        \n",
    "        for batch_start in range(resume_index, total_articles, checkpoint):\n",
    "            batch_end = min(batch_start + checkpoint, total_articles)\n",
    "            batch_indices = list(range(batch_start, batch_end))\n",
    "            \n",
    "            tasks = [(idx, df.at[idx, \"link\"]) for idx in batch_indices]\n",
    "            results = []\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                future_to_idx = {executor.submit(process_article_bs, idx, url): idx for idx, url in tasks}\n",
    "                for future in concurrent.futures.as_completed(future_to_idx):\n",
    "                    idx_processed, text = future.result()\n",
    "                    results.append((idx_processed, text))\n",
    "                    if (idx_processed + 1) % progress_interval == 0:\n",
    "                        print(f\"Processing article {idx_processed+1}: {df.at[idx_processed, 'link'][:50]}...\")\n",
    "            \n",
    "            results.sort(key=lambda x: x[0])\n",
    "            for idx_processed, text in results:\n",
    "                category = df.at[idx_processed, \"category\"]\n",
    "                cat_esc = category.replace('\"', '\"\"')\n",
    "                text_esc = text.replace('\"', '\"\"')\n",
    "                writer.writerow([cat_esc, text_esc])\n",
    "            \n",
    "            fout.flush()  \n",
    "            save_resume_index(last_index_file, batch_end)\n",
    "            print(f\"Checkpoint: {batch_end} articole procesate. Salvare completă pentru acest batch.\")\n",
    "            time.sleep(5)  \n",
    "    \n",
    "    save_resume_index(last_index_file, total_articles)\n",
    "    print(f\"Procesare completă! Articole totale: {total_articles}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proiect_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
